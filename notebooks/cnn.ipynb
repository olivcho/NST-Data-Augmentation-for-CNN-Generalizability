{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw: 858 images\n",
      "gas_pump: 931 images\n",
      "tench: 963 images\n",
      "french_horn: 956 images\n",
      "church: 941 images\n",
      "english_springer: 955 images\n",
      "golf_ball: 951 images\n",
      "garbage_truck: 961 images\n",
      "parachute: 960 images\n",
      "cassette_player: 993 images\n",
      "\n",
      "Total images across all folders: 9469\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/source_images\"\n",
    "\n",
    "total_images = 0\n",
    "for folder in os.listdir(data_path):\n",
    "    if os.path.isdir(os.path.join(data_path, folder)):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        print(f\"{folder}: {len(images)} images\")\n",
    "        total_images += len(images)\n",
    "print(f\"\\nTotal images across all folders: {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/source_images\"\n",
    "\n",
    "def rename_images(base_path):\n",
    "\n",
    "    subfolders = [f for f in os.listdir(base_path)]\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(base_path, subfolder)\n",
    "\n",
    "        files = glob.glob(os.path.join(subfolder_path, \"*\"))\n",
    "\n",
    "        for i, file_path in enumerate(files):\n",
    "            _, ext = os.path.splitext(file_path)\n",
    "            new_path = os.path.join(subfolder_path, f\"{subfolder}_{i}{ext}\")\n",
    "            os.rename(file_path, new_path)\n",
    "\n",
    "rename_images(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chainsaw: 100%|██████████| 858/858 [00:00<00:00, 1052.52it/s]\n",
      "Processing gas_pump: 100%|██████████| 931/931 [00:01<00:00, 771.79it/s]\n",
      "Processing tench: 100%|██████████| 963/963 [00:01<00:00, 941.83it/s]\n",
      "Processing french_horn: 100%|██████████| 956/956 [00:00<00:00, 1322.82it/s]\n",
      "Processing church: 100%|██████████| 941/941 [00:00<00:00, 1218.35it/s]\n",
      "Processing english_springer: 100%|██████████| 955/955 [00:00<00:00, 1198.13it/s]\n",
      "Processing golf_ball: 100%|██████████| 951/951 [00:01<00:00, 715.09it/s]\n",
      "Processing garbage_truck: 100%|██████████| 961/961 [00:01<00:00, 685.38it/s]\n",
      "Processing parachute: 100%|██████████| 960/960 [00:01<00:00, 733.18it/s]\n",
      "Processing cassette_player: 100%|██████████| 993/993 [00:01<00:00, 749.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: torch.Size([7575, 3, 64, 64])\n",
      "Validation set shape: torch.Size([947, 3, 64, 64])\n",
      "Test set shape: torch.Size([947, 3, 64, 64])\n",
      "\n",
      "Class mapping: {'chainsaw': 0, 'gas_pump': 1, 'tench': 2, 'french_horn': 3, 'church': 4, 'english_springer': 5, 'golf_ball': 6, 'garbage_truck': 7, 'parachute': 8, 'cassette_player': 9}\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_images(data_path, target_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    \n",
    "    # Get all subfolders (classes)\n",
    "    subfolders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "    \n",
    "    # Create label mapping\n",
    "    for idx, folder in enumerate(subfolders):\n",
    "        label_map[folder] = idx\n",
    "    \n",
    "    # Load and preprocess images from each subfolder\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(data_path, subfolder)\n",
    "        files = glob.glob(os.path.join(subfolder_path, \"*\"))\n",
    "        \n",
    "        for file_path in tqdm.tqdm(files, desc=f\"Processing {subfolder}\"):\n",
    "            # Load image\n",
    "            img = Image.open(file_path).convert('RGB')  # Ensure RGB format\n",
    "            \n",
    "            # Resize \n",
    "            img = img.resize(target_size, Image.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array and normalize\n",
    "            img_array = np.array(img) / 255.0\n",
    "            \n",
    "            images.append(img_array)\n",
    "            labels.append(label_map[subfolder])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Reshape for PyTorch (N, C, H, W)\n",
    "    X = X.transpose(0, 3, 1, 2)  # Rearrange from (N, H, W, C) to (N, C, H, W)\n",
    "    \n",
    "    return X, y, label_map\n",
    "\n",
    "# Load and preprocess all images\n",
    "X, y, label_map = load_and_preprocess_images(data_path)\n",
    "\n",
    "# First split into train and temp (80-20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split temp into validation and test (50-50, meaning 10-10 of original data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nClass mapping: {label_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7575, 3, 64, 64, 947, 3, 64, 64, 947, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape + X_test.shape + X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectRecognition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObjectRecognition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 32x32x32\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64x16x16\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 128x8x8\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(8192, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10) # 10 classes\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: Bx3x64x64\n",
    "        x = self.mp1(self.bn1(self.relu(self.conv1(x))))  # Bx32x32x32\n",
    "        x = self.mp2(self.bn2(self.relu(self.conv2(x))))  # Bx64x16x16\n",
    "        x = self.mp3(self.bn3(self.relu(self.conv3(x))))  # Bx128x8x8\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Bx(128*8*8)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))  # Bx512\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))  # Bx128\n",
    "        x = self.fc3(x)  # Bx10\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectRecognition()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8:   9%|▉         | 21/237 [00:02<00:24,  8.94it/s, loss=0.487]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mObjectRecognition.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Input shape: Bx3x64x64\u001b[39;00m\n\u001b[32m     25\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.mp1(\u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv1(x))))  \u001b[38;5;66;03m# Bx32x32x32\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.mp2(\u001b[38;5;28mself\u001b[39m.bn2(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))  \u001b[38;5;66;03m# Bx64x16x16\u001b[39;00m\n\u001b[32m     27\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.mp3(\u001b[38;5;28mself\u001b[39m.bn3(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))))  \u001b[38;5;66;03m# Bx128x8x8\u001b[39;00m\n\u001b[32m     29\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Bx(128*8*8)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Progress bar for training batches\n",
    "    progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to /Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/model_weights_15_epochs.pth\n"
     ]
    }
   ],
   "source": [
    "# # Save model weights\n",
    "# model_save_path = '/Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/model_weights_15_epochs.pth'\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "# print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from /Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object-detection-nonadversarial/model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Load saved model weights\n",
    "# model_weights_path = '/Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object-detection-nonadversarial/model_weights.pth'\n",
    "# model.load_state_dict(torch.load(model_weights_path))\n",
    "# print(f\"Model weights loaded from {model_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.8033\n",
      "Test Accuracy: 0.3178\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    return {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': correct / total,\n",
    "    }\n",
    "\n",
    "# Evaluate model on test set\n",
    "test_results = evaluate_model(model, test_loader)\n",
    "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class Performance Metrics:\n",
      "              Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "0          chainsaw      0.500   0.465     0.482       86     0.465\n",
      "1          gas_pump      0.560   0.602     0.580       93     0.602\n",
      "2             tench      0.758   0.742     0.750       97     0.742\n",
      "3       french_horn      0.630   0.842     0.721       95     0.842\n",
      "4            church      0.742   0.766     0.754       94     0.766\n",
      "5  english_springer      0.704   0.719     0.711       96     0.719\n",
      "6         golf_ball      0.696   0.579     0.632       95     0.579\n",
      "7     garbage_truck      0.703   0.740     0.721       96     0.740\n",
      "8         parachute      0.848   0.812     0.830       96     0.812\n",
      "9   cassette_player      0.782   0.616     0.689       99     0.616\n",
      "\n",
      "Macro Averages:\n",
      "Precision: 0.692\n",
      "Recall: 0.688\n",
      "F1-Score: 0.687\n",
      "Accuracy: 0.688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "class_names = ['chainsaw', 'gas_pump', 'tench', 'french_horn', 'church', 'english_springer', 'golf_ball', 'garbage_truck', 'parachute', 'cassette_player']\n",
    "\n",
    "# Get predictions and true labels for entire test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "# Calculate metrics for each class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, \n",
    "    all_preds,\n",
    "    labels=range(len(class_names)),\n",
    "    average=None\n",
    ")\n",
    "\n",
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Calculate accuracy for each class from confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "results_df['Accuracy'] = class_accuracy\n",
    "\n",
    "# Display results with nice formatting\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "print(\"\\nPer-class Performance Metrics:\")\n",
    "print(results_df)\n",
    "\n",
    "# Calculate and display macro averages\n",
    "macro_avg = results_df[['Precision', 'Recall', 'F1-Score', 'Accuracy']].mean()\n",
    "print(\"\\nMacro Averages:\")\n",
    "for metric, value in macro_avg.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to /Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/model_weights_8_epochs.pth\n"
     ]
    }
   ],
   "source": [
    "# # Save model weights\n",
    "model_save_path = '/Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/model_weights_8_epochs.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylized Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating chainsaw...\n",
      "\n",
      "Evaluating gas_pump...\n",
      "\n",
      "Evaluating tench...\n",
      "\n",
      "Evaluating french_horn...\n",
      "\n",
      "Evaluating church...\n",
      "\n",
      "Evaluating english_springer...\n",
      "\n",
      "Evaluating golf_ball...\n",
      "\n",
      "Evaluating garbage_truck...\n",
      "\n",
      "Evaluating parachute...\n",
      "\n",
      "Evaluating cassette_player...\n",
      "\n",
      "Evaluation Results:\n",
      "\n",
      "chainsaw:\n",
      "Accuracy: 2.38%\n",
      "Average Confidence: 85.20%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 30.9% of predictions\n",
      "  - parachute: 24.3% of predictions\n",
      "  - tench: 15.2% of predictions\n",
      "\n",
      "gas_pump:\n",
      "Accuracy: 0.00%\n",
      "Average Confidence: 86.70%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 31.9% of predictions\n",
      "  - parachute: 25.6% of predictions\n",
      "  - tench: 18.3% of predictions\n",
      "\n",
      "tench:\n",
      "Accuracy: 10.52%\n",
      "Average Confidence: 85.35%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 32.1% of predictions\n",
      "  - tench: 30.6% of predictions\n",
      "  - parachute: 24.5% of predictions\n",
      "\n",
      "french_horn:\n",
      "Accuracy: 9.20%\n",
      "Average Confidence: 88.03%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 32.2% of predictions\n",
      "  - parachute: 23.1% of predictions\n",
      "  - tench: 18.7% of predictions\n",
      "\n",
      "church:\n",
      "Accuracy: 0.94%\n",
      "Average Confidence: 86.87%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 32.2% of predictions\n",
      "  - parachute: 26.7% of predictions\n",
      "  - tench: 19.5% of predictions\n",
      "\n",
      "english_springer:\n",
      "Accuracy: 12.66%\n",
      "Average Confidence: 89.74%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 32.3% of predictions\n",
      "  - parachute: 21.1% of predictions\n",
      "  - english_springer: 20.7% of predictions\n",
      "\n",
      "golf_ball:\n",
      "Accuracy: 89.96%\n",
      "Average Confidence: 92.81%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 33.2% of predictions\n",
      "  - parachute: 28.0% of predictions\n",
      "  - tench: 20.9% of predictions\n",
      "\n",
      "garbage_truck:\n",
      "Accuracy: 1.24%\n",
      "Average Confidence: 86.91%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 31.7% of predictions\n",
      "  - parachute: 24.9% of predictions\n",
      "  - tench: 12.9% of predictions\n",
      "\n",
      "parachute:\n",
      "Accuracy: 30.65%\n",
      "Average Confidence: 87.54%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 32.8% of predictions\n",
      "  - parachute: 30.0% of predictions\n",
      "  - tench: 15.7% of predictions\n",
      "\n",
      "cassette_player:\n",
      "Accuracy: 1.88%\n",
      "Average Confidence: 87.33%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 30.8% of predictions\n",
      "  - parachute: 21.3% of predictions\n",
      "  - cassette_player: 16.2% of predictions\n"
     ]
    }
   ],
   "source": [
    "class_names = list(label_map.keys())\n",
    "\n",
    "# Create a dictionary to store results for each class\n",
    "results = {class_name: [] for class_name in class_names}\n",
    "\n",
    "# Process each class\n",
    "for class_name in class_names:\n",
    "    # Get all images for this class from processed folder\n",
    "    class_path = os.path.join('/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/processed', class_name)\n",
    "    images = glob.glob(os.path.join(class_path, '*.jpg'))\n",
    "    \n",
    "    print(f\"\\nEvaluating {class_name}...\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in list(images):  # Convert glob iterator to list before iterating\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize((64, 64), Image.LANCZOS)\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.transpose(2, 0, 1)).float()\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Get model prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "            # Get top 3 predictions\n",
    "            top3_prob, top3_idx = torch.topk(probs, 3)\n",
    "            \n",
    "        results[class_name].append({\n",
    "            'top3_classes': [class_names[idx] for idx in top3_idx],\n",
    "            'top3_confidences': [prob.item() for prob in top3_prob]\n",
    "        })\n",
    "\n",
    "# Print summary for each class\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for class_name, predictions in results.items():\n",
    "    total = len(predictions)\n",
    "    \n",
    "    # Calculate accuracy (when true class is top prediction)\n",
    "    correct = sum(1 for p in predictions if p['top3_classes'][0] == class_name)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate average confidence of top prediction\n",
    "    avg_confidence = sum(p['top3_confidences'][0] for p in predictions) / total\n",
    "    \n",
    "    # Count frequency of each predicted class\n",
    "    prediction_counts = {}\n",
    "    for p in predictions:\n",
    "        for pred_class in p['top3_classes']:\n",
    "            prediction_counts[pred_class] = prediction_counts.get(pred_class, 0) + 1\n",
    "    \n",
    "    # Get top 3 most common predictions\n",
    "    most_common = sorted(prediction_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Average Confidence: {avg_confidence:.2%}\")\n",
    "    print(\"Most commonly predicted as:\")\n",
    "    for pred_class, count in most_common:\n",
    "        percentage = count / (total * 3) * 100  # Divide by total*3 since each image has 3 predictions\n",
    "        print(f\"  - {pred_class}: {percentage:.1f}% of predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training dataset 50% original and 50% stylized. Validate on 100% original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/source_images\"\n",
    "stylized_data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/processed\"\n",
    "\n",
    "X_orig, y_orig, label_map = load_and_preprocess_images(original_data_path)\n",
    "X_style, y_style, _ = load_and_preprocess_images(stylized_data_path)\n",
    "\n",
    "print(f\"Original data: {len(X_orig)} images\")\n",
    "print(f\"Stylized data: {len(X_style)} images\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly subsample stylized data to match original data size\n",
    "subset_size = len(X_orig)\n",
    "style_indices = np.random.choice(len(X_style), subset_size, replace=False)\n",
    "X_style_subset = X_style[style_indices]\n",
    "y_style_subset = y_style[style_indices]\n",
    "\n",
    "print(f\"Stylized subset: {len(X_style_subset)} images (randomly sampled)\")\n",
    "\n",
    "# Split original data into train, val and test\n",
    "X_orig_train, X_orig_temp, y_orig_train, y_orig_temp = train_test_split(\n",
    "    X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig\n",
    ")\n",
    "X_orig_val, X_orig_test, y_orig_val, y_orig_test = train_test_split(\n",
    "    X_orig_temp, y_orig_temp, test_size=0.5, random_state=42, stratify=y_orig_temp\n",
    ")\n",
    "\n",
    "# Split stylized subset - we only need training portion\n",
    "X_style_train, _, y_style_train, _ = train_test_split(\n",
    "    X_style_subset, y_style_subset, test_size=0.2, random_state=42, stratify=y_style_subset\n",
    ")\n",
    "\n",
    "# Combine original and stylized for training (true 50-50 split)\n",
    "X_train = np.concatenate([X_orig_train, X_style_train])\n",
    "y_train = np.concatenate([y_orig_train, y_style_train])\n",
    "\n",
    "# Use original data only for validation and test\n",
    "X_val = X_orig_val\n",
    "y_val = y_orig_val\n",
    "X_test = X_orig_test \n",
    "y_test = y_orig_test\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"\\nFinal dataset sizes:\")\n",
    "print(f\"Training set shape: {X_train.shape} (50% original: {len(X_orig_train)}, 50% stylized: {len(X_style_train)})\")\n",
    "print(f\"Validation set shape: {X_val.shape} (100% original)\")\n",
    "print(f\"Test set shape: {X_test.shape} (100% original)\")\n",
    "print(f\"\\nClass mapping: {label_map}\")\n",
    "\n",
    "# Verify class balance in training set\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique_train, counts_train = np.unique(y_train.numpy(), return_counts=True)\n",
    "for class_idx, count in zip(unique_train, counts_train):\n",
    "    class_name = [k for k, v in label_map.items() if v == class_idx][0]\n",
    "    print(f\"  {class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chainsaw: 100%|██████████| 858/858 [00:01<00:00, 577.52it/s]\n",
      "Processing gas_pump: 100%|██████████| 931/931 [00:01<00:00, 558.92it/s]\n",
      "Processing tench: 100%|██████████| 963/963 [00:01<00:00, 596.21it/s]\n",
      "Processing french_horn: 100%|██████████| 956/956 [00:01<00:00, 538.32it/s]\n",
      "Processing church: 100%|██████████| 941/941 [00:01<00:00, 590.70it/s]\n",
      "Processing english_springer: 100%|██████████| 955/955 [00:01<00:00, 646.24it/s]\n",
      "Processing golf_ball: 100%|██████████| 951/951 [00:01<00:00, 586.91it/s]\n",
      "Processing garbage_truck: 100%|██████████| 961/961 [00:01<00:00, 529.48it/s]\n",
      "Processing parachute: 100%|██████████| 960/960 [00:01<00:00, 527.25it/s]\n",
      "Processing cassette_player: 100%|██████████| 993/993 [00:01<00:00, 563.17it/s]\n",
      "Processing chainsaw: 100%|██████████| 2147/2147 [00:05<00:00, 390.11it/s]\n",
      "Processing gas_pump: 100%|██████████| 2340/2340 [00:05<00:00, 427.02it/s]\n",
      "Processing tench: 100%|██████████| 2405/2405 [00:06<00:00, 375.21it/s]\n",
      "Processing french_horn: 100%|██████████| 2425/2425 [00:05<00:00, 436.69it/s]\n",
      "Processing church: 100%|██████████| 2335/2335 [00:05<00:00, 425.38it/s]\n",
      "Processing english_springer: 100%|██████████| 2370/2370 [00:04<00:00, 475.22it/s]\n",
      "Processing golf_ball: 100%|██████████| 2380/2380 [00:06<00:00, 391.31it/s]\n",
      "Processing garbage_truck: 100%|██████████| 2425/2425 [00:05<00:00, 409.61it/s]\n",
      "Processing parachute: 100%|██████████| 2450/2450 [00:05<00:00, 456.45it/s]\n",
      "Processing cassette_player: 100%|██████████| 2500/2500 [00:05<00:00, 438.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: torch.Size([26596, 3, 64, 64]) (50% original, 50% stylized)\n",
      "Validation set shape: torch.Size([947, 3, 64, 64]) (100% original)\n",
      "Test set shape: torch.Size([947, 3, 64, 64]) (100% original)\n",
      "\n",
      "Class mapping: {'chainsaw': 0, 'gas_pump': 1, 'tench': 2, 'french_horn': 3, 'church': 4, 'english_springer': 5, 'golf_ball': 6, 'garbage_truck': 7, 'parachute': 8, 'cassette_player': 9}\n"
     ]
    }
   ],
   "source": [
    "original_data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/source_images\"\n",
    "stylized_data_path = \"/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/processed\"\n",
    "\n",
    "X_orig, y_orig, label_map = load_and_preprocess_images(original_data_path)\n",
    "X_style, y_style, _ = load_and_preprocess_images(stylized_data_path)\n",
    "\n",
    "# Split original data into train, val and test\n",
    "X_orig_train, X_orig_temp, y_orig_train, y_orig_temp = train_test_split(\n",
    "    X_orig, y_orig, test_size=0.2, random_state=42, stratify=y_orig\n",
    ")\n",
    "X_orig_val, X_orig_test, y_orig_val, y_orig_test = train_test_split(\n",
    "    X_orig_temp, y_orig_temp, test_size=0.5, random_state=42, stratify=y_orig_temp\n",
    ")\n",
    "\n",
    "# Split stylized data - we only need training portion\n",
    "X_style_train, _, y_style_train, _ = train_test_split(\n",
    "    X_style, y_style, test_size=0.2, random_state=42, stratify=y_style\n",
    ")\n",
    "\n",
    "# Combine original and stylized for training (50-50 split)\n",
    "X_train = np.concatenate([X_orig_train, X_style_train])\n",
    "y_train = np.concatenate([y_orig_train, y_style_train])\n",
    "\n",
    "# Use original data only for validation and test\n",
    "X_val = X_orig_val\n",
    "y_val = y_orig_val\n",
    "X_test = X_orig_test \n",
    "y_test = y_orig_test\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape} (50% original, 50% stylized)\")\n",
    "print(f\"Validation set shape: {X_val.shape} (100% original)\")\n",
    "print(f\"Test set shape: {X_test.shape} (100% original)\")\n",
    "print(f\"\\nClass mapping: {label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|██████████| 832/832 [01:34<00:00,  8.77it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8] - Loss: 1.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|██████████| 832/832 [01:27<00:00,  9.54it/s, loss=1.16] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8] - Loss: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|██████████| 832/832 [01:28<00:00,  9.37it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8] - Loss: 0.5835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|██████████| 832/832 [01:29<00:00,  9.33it/s, loss=0.0237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8] - Loss: 0.4058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|██████████| 832/832 [01:36<00:00,  8.62it/s, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8] - Loss: 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|██████████| 832/832 [01:36<00:00,  8.59it/s, loss=0.668] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8] - Loss: 0.2192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|██████████| 832/832 [01:28<00:00,  9.41it/s, loss=0.00139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8] - Loss: 0.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|██████████| 832/832 [01:28<00:00,  9.45it/s, loss=0.257]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8] - Loss: 0.1556\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Progress bar for training batches\n",
    "    progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to /Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/50-50-model-weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model weights\n",
    "model_save_path = '/Users/olicho/Desktop/Programming/adversarial-style-transfer/models-weights/object_detection/50-50-model-weights.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1954\n",
      "Test Accuracy: 0.8046\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_model(model, test_loader)\n",
    "print(f\"Test Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class Performance Metrics:\n",
      "              Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "0          chainsaw      0.771   0.628     0.692       86     0.628\n",
      "1          gas_pump      0.694   0.806     0.746       93     0.806\n",
      "2             tench      0.933   0.866     0.898       97     0.866\n",
      "3       french_horn      0.822   0.779     0.800       95     0.779\n",
      "4            church      0.869   0.777     0.820       94     0.777\n",
      "5  english_springer      0.828   0.802     0.815       96     0.802\n",
      "6         golf_ball      0.816   0.842     0.829       95     0.842\n",
      "7     garbage_truck      0.717   0.844     0.775       96     0.844\n",
      "8         parachute      0.856   0.865     0.860       96     0.865\n",
      "9   cassette_player      0.779   0.818     0.798       99     0.818\n",
      "\n",
      "Macro Averages:\n",
      "Precision: 0.809\n",
      "Recall: 0.803\n",
      "F1-Score: 0.803\n",
      "Accuracy: 0.803\n"
     ]
    }
   ],
   "source": [
    "class_names = ['chainsaw', 'gas_pump', 'tench', 'french_horn', 'church', 'english_springer', 'golf_ball', 'garbage_truck', 'parachute', 'cassette_player']\n",
    "\n",
    "# Get predictions and true labels for entire test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "# Calculate metrics for each class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, \n",
    "    all_preds,\n",
    "    labels=range(len(class_names)),\n",
    "    average=None\n",
    ")\n",
    "\n",
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "# Calculate accuracy for each class from confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "results_df['Accuracy'] = class_accuracy\n",
    "\n",
    "# Display results with nice formatting\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "print(\"\\nPer-class Performance Metrics:\")\n",
    "print(results_df)\n",
    "\n",
    "# Calculate and display macro averages\n",
    "macro_avg = results_df[['Precision', 'Recall', 'F1-Score', 'Accuracy']].mean()\n",
    "print(\"\\nMacro Averages:\")\n",
    "for metric, value in macro_avg.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating chainsaw...\n",
      "\n",
      "Evaluating gas_pump...\n",
      "\n",
      "Evaluating tench...\n",
      "\n",
      "Evaluating french_horn...\n",
      "\n",
      "Evaluating church...\n",
      "\n",
      "Evaluating english_springer...\n",
      "\n",
      "Evaluating golf_ball...\n",
      "\n",
      "Evaluating garbage_truck...\n",
      "\n",
      "Evaluating parachute...\n",
      "\n",
      "Evaluating cassette_player...\n",
      "\n",
      "Evaluation Results:\n",
      "\n",
      "chainsaw:\n",
      "Accuracy: 99.12%\n",
      "Average Confidence: 97.96%\n",
      "Most commonly predicted as:\n",
      "  - chainsaw: 33.3% of predictions\n",
      "  - parachute: 11.1% of predictions\n",
      "  - golf_ball: 10.8% of predictions\n",
      "\n",
      "gas_pump:\n",
      "Accuracy: 98.89%\n",
      "Average Confidence: 96.78%\n",
      "Most commonly predicted as:\n",
      "  - gas_pump: 33.2% of predictions\n",
      "  - chainsaw: 12.5% of predictions\n",
      "  - church: 11.3% of predictions\n",
      "\n",
      "tench:\n",
      "Accuracy: 99.58%\n",
      "Average Confidence: 99.06%\n",
      "Most commonly predicted as:\n",
      "  - tench: 33.3% of predictions\n",
      "  - golf_ball: 17.8% of predictions\n",
      "  - french_horn: 10.9% of predictions\n",
      "\n",
      "french_horn:\n",
      "Accuracy: 99.09%\n",
      "Average Confidence: 96.74%\n",
      "Most commonly predicted as:\n",
      "  - french_horn: 33.3% of predictions\n",
      "  - golf_ball: 14.7% of predictions\n",
      "  - cassette_player: 12.5% of predictions\n",
      "\n",
      "church:\n",
      "Accuracy: 99.06%\n",
      "Average Confidence: 97.54%\n",
      "Most commonly predicted as:\n",
      "  - church: 33.3% of predictions\n",
      "  - parachute: 15.0% of predictions\n",
      "  - golf_ball: 10.4% of predictions\n",
      "\n",
      "english_springer:\n",
      "Accuracy: 98.90%\n",
      "Average Confidence: 96.82%\n",
      "Most commonly predicted as:\n",
      "  - english_springer: 33.3% of predictions\n",
      "  - golf_ball: 14.2% of predictions\n",
      "  - chainsaw: 14.2% of predictions\n",
      "\n",
      "golf_ball:\n",
      "Accuracy: 99.62%\n",
      "Average Confidence: 98.19%\n",
      "Most commonly predicted as:\n",
      "  - golf_ball: 33.3% of predictions\n",
      "  - parachute: 16.4% of predictions\n",
      "  - french_horn: 9.9% of predictions\n",
      "\n",
      "garbage_truck:\n",
      "Accuracy: 99.30%\n",
      "Average Confidence: 98.32%\n",
      "Most commonly predicted as:\n",
      "  - garbage_truck: 33.3% of predictions\n",
      "  - chainsaw: 12.6% of predictions\n",
      "  - cassette_player: 11.6% of predictions\n",
      "\n",
      "parachute:\n",
      "Accuracy: 99.31%\n",
      "Average Confidence: 98.12%\n",
      "Most commonly predicted as:\n",
      "  - parachute: 33.3% of predictions\n",
      "  - golf_ball: 22.0% of predictions\n",
      "  - chainsaw: 12.2% of predictions\n",
      "\n",
      "cassette_player:\n",
      "Accuracy: 99.12%\n",
      "Average Confidence: 97.82%\n",
      "Most commonly predicted as:\n",
      "  - cassette_player: 33.3% of predictions\n",
      "  - garbage_truck: 16.0% of predictions\n",
      "  - golf_ball: 15.2% of predictions\n"
     ]
    }
   ],
   "source": [
    "class_names = list(label_map.keys())\n",
    "\n",
    "# Create a dictionary to store results for each class\n",
    "results = {class_name: [] for class_name in class_names}\n",
    "\n",
    "# Process each class\n",
    "for class_name in class_names:\n",
    "    # Get all images for this class from processed folder\n",
    "    class_path = os.path.join('/Users/olicho/Desktop/Programming/adversarial-style-transfer/data/processed', class_name)\n",
    "    images = glob.glob(os.path.join(class_path, '*.jpg'))\n",
    "    \n",
    "    print(f\"\\nEvaluating {class_name}...\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in list(images):  # Convert glob iterator to list before iterating\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize((64, 64), Image.LANCZOS)\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array.transpose(2, 0, 1)).float()\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Get model prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "            # Get top 3 predictions\n",
    "            top3_prob, top3_idx = torch.topk(probs, 3)\n",
    "            \n",
    "        results[class_name].append({\n",
    "            'top3_classes': [class_names[idx] for idx in top3_idx],\n",
    "            'top3_confidences': [prob.item() for prob in top3_prob]\n",
    "        })\n",
    "\n",
    "# Print summary for each class\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for class_name, predictions in results.items():\n",
    "    total = len(predictions)\n",
    "    \n",
    "    # Calculate accuracy (when true class is top prediction)\n",
    "    correct = sum(1 for p in predictions if p['top3_classes'][0] == class_name)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate average confidence of top prediction\n",
    "    avg_confidence = sum(p['top3_confidences'][0] for p in predictions) / total\n",
    "    \n",
    "    # Count frequency of each predicted class\n",
    "    prediction_counts = {}\n",
    "    for p in predictions:\n",
    "        for pred_class in p['top3_classes']:\n",
    "            prediction_counts[pred_class] = prediction_counts.get(pred_class, 0) + 1\n",
    "    \n",
    "    # Get top 3 most common predictions\n",
    "    most_common = sorted(prediction_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Average Confidence: {avg_confidence:.2%}\")\n",
    "    print(\"Most commonly predicted as:\")\n",
    "    for pred_class, count in most_common:\n",
    "        percentage = count / (total * 3) * 100  # Divide by total*3 since each image has 3 predictions\n",
    "        print(f\"  - {pred_class}: {percentage:.1f}% of predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
